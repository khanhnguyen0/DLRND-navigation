{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution description\n",
    "### Deep Q Network\n",
    "In order to find the optimal policy for this project, I used a deep neural network to approximate the Q value function. The activation of each layer(except the output) is ReLU and a dropout probability of 0.1 is applied.\n",
    "To solve the Unity environment, I used the deep Q network with the following hyper parameters: \n",
    "```python\n",
    "NETWORK_LINEAR_SIZES = \"1024,512,256\" # dimension for every layer in Q network\n",
    "```\n",
    "\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "\n",
    "### Experience replay buffer\n",
    "An experience replay buffer with size of `1e5` is used to store the experience tuple. Each tuple contains the input state, the chosen action, the corresponding reward from the environment and the next state. For each training step, 64 tuples will be randomly sampled from the buffer to train the Q network.\n",
    "\n",
    "### epsilon-greedy action selection\n",
    "At every timestep, the agent has a probability equal to the epsilon to select a random action from the action state (explore) instead of the action with the highest Q value (exploit). epsilon-greedy action selection is used to encourage the agent to explore the environment more at the start of the training session. The epsilon is intialized at `1.0` and will be reduced after every episode, down to the minimum value `0.01`.\n",
    "\n",
    "### Temporal difference with fixed Q target\n",
    "The agent is trained with temporal difference algorithm with fixed Q target technique, which means 2 Q networks, the target and the local are trained to approximate the Q function. At every train step, a batch of experience tuples are randomly sampled from the replay buffer. The target value is calculated by feeding the next state to the target network and take the action with the highest output value. The input state is fed into the local network, calculate the Q value for each action and update the action selected in this tuple using the target value. The local network is updated at every train step while the target network is only updated after a number of steps (4 in this project) using soft update, which is controlled by the hyper parameter `τ`. Simply put, the parameters of the target network is updated by the following formula at every 4 train step: `θ−=θ-×τ+θ×(1−τ)`. Where `θ−` is the target network parameter and `θ` is the local network parameter. For this project, I set the value of `τ` at `1e-3`.\n",
    "\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "\n",
    "## Results\n",
    "The agent was able to solve the environment (average score >= 13) after 450 episode:\n",
    "![alt text](https://i.ibb.co/31Vnt38/Figure-1.png \"Average agent score\")\n",
    "\n",
    "## Future improvement\n",
    "Use Q network with more layers, apply Double DQN, Prioritized Experience Replay and Dueling DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
