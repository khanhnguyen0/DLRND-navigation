{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the Unity environment, I used the deep Q network with the following hyper parameters: \n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "For approximating the Q function, a neural with 3 layers and size of 1024, 512 and 256 is used. The activation of each layer(except the output) is ReLU and a dropout probability of 0.1 is applied:\n",
    "```python\n",
    "NETWORK_LINEAR_SIZES = \"1024,512,256\" # dimension for every layer in Q network\n",
    "```\n",
    "The agent was able to solve the environment (average score >= 13) after 450 episode:\n",
    "![alt text](https://i.ibb.co/31Vnt38/Figure-1.png \"Average agent score\")\n",
    "\n",
    "## Future improvement\n",
    "Use Q network with more layers, apply Double DQN, Prioritized Experience Replay and Dueling DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
